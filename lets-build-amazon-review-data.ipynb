{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Build: A Machine Learning Algorithm that Determines Review Sentiment (Using Python 3)\n",
    "\n",
    "Code for [Let's Build Blog Post](https://www.mthree.com/news/let-s-build-a-machine-learning-algorithm-that-determines-review-sentiment-using-python-3/41573/)\n",
    "\n",
    "To be the first to know about our future 'Let's Build' walkthroughs, follow us on [LinkedIn](https://www.linkedin.com/company/mthree/), [Instagram](https://www.instagram.com/mthreeconsulting/), [Twitter](https://twitter.com/MthreeC) and [Facebook](https://www.facebook.com/mthreealumni/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Verify and load required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load appropriate packages \n",
    "import nltk\n",
    "import tensorflow as tf  \n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import json\n",
    "import sklearn\n",
    "import matplotlib\n",
    "print(\"Ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Import the data and create the datasets\n",
    "\n",
    "Dataset: [Kindle Store 5-core](http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Kindle_Store_5.json.gz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import gzip\n",
    "import pprint\n",
    "dataset1 = list()\n",
    "dataset2 = list()\n",
    "def parse(path):\n",
    "    g = gzip.open(path, 'r')\n",
    "    for l in g:\n",
    "        yield json.loads(l)\n",
    "# update path for local file as necessary\n",
    "for l in parse(\"reviews_Kindle_Store_5.json.gz\"):\n",
    "    if int(l['overall']) == 1 or int(l['overall']) == 2:\n",
    "        dataset1.append(l)\n",
    "    if int(l['overall']) == 5:\n",
    "        dataset2.append(l)\n",
    "\n",
    "dataset1 = dataset1[:50000]\n",
    "print(\"Number of negative reviews in the dataset: \" + str(len(dataset1)))\n",
    "print(\"Example of a negative review:\")\n",
    "pprint.pprint(dataset1[0])\n",
    "\n",
    "dataset2 = dataset2[:50000]\n",
    "print(\"\\nNumber of positive reviews in the dataset: \" + str(len(dataset2)))\n",
    "print(\"Example of a positive review:\")\n",
    "pprint.pprint(dataset2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a single dataset with all collected reviews\n",
    "\n",
    "#dataset1: list of negative reviews \n",
    "#dataset2: list of positive reviews \n",
    "dataset = dataset1 + dataset2\n",
    "print(\"Number of reviews in the dataset: \" + str(len(dataset)))\n",
    "print(\"\\nFirst record in dataset:\")\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Split the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into two different lists.\n",
    "dataset_labels = list()\n",
    "dataset_text = list()\n",
    "\n",
    "num_reviews_positive  = 0\n",
    "num_reviews_negative  = 0\n",
    "\n",
    "# Iterate through the dataset\n",
    "for row in dataset:\n",
    "    if int(row['overall']) == 1 or int(row['overall']) == 2:\n",
    "        dataset_labels.append(0) # negative: 0 \n",
    "        num_reviews_negative = num_reviews_negative + 1   \n",
    "    elif int(row['overall']) == 5:\n",
    "        dataset_labels.append(1) # positive = 1  \n",
    "        num_reviews_positive = num_reviews_positive + 1   \n",
    "    else:\n",
    "        continue\n",
    "    dataset_text.append(row['reviewText'])\n",
    "    \n",
    "print(\"Number of negative reviews: \" + str(num_reviews_negative))\n",
    "print(\"Number of positive reviews: \" + str(num_reviews_negative))\n",
    "\n",
    "print(\"\\nFirst record in dataset:\")\n",
    "# Display the text of a review\n",
    "print(dataset_text[0]) #X\n",
    "# Display the label assigned to the review\n",
    "print(dataset_labels[0]) #Y\n",
    "\n",
    "print(\"\\nSecond record in dataset:\")\n",
    "print(dataset_text[1]) #X\n",
    "print(dataset_labels[1]) #Y\n",
    "\n",
    "# Machine learning \n",
    "# F(X) = Y : F has parameters: Estimate the parameters? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Convert text to integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the imdb dataset' index values to map our reviews dataset\n",
    "imdb = keras.datasets.imdb\n",
    "\n",
    "# Create a word_index dictionary where the key is a word and the value is an index of that token,\n",
    "# using a prebuilt word_index dictionary from the imdb database\n",
    "word_index = imdb.get_word_index()\n",
    "\n",
    "# Add these key,value pairs to the word_index dictionary for padding\n",
    "word_index[\"<PAD>\"] = 0\n",
    "\n",
    "# Because of the padding, we offset the original index values by 3\n",
    "word_index = {k:(v + 3) for k,v in word_index.items()} \n",
    "\n",
    "print(\"Ready\")\n",
    "\n",
    "print(\"Index value for 'ready': \" + str(word_index[\"ready\"]))\n",
    "print(\"Index value for 'enjoy': \" + str(word_index[\"enjoy\"]))\n",
    "print(\"Index value for 'enjoying': \" + str(word_index[\"enjoying\"]))\n",
    "print(\"Index value for 'john': \" + str(word_index[\"john\"]))\n",
    "\n",
    "print(\"Number of items in index: \" +str(len(word_index.keys())))\n",
    "word_index[\"<PAD>\"]\n",
    " \n",
    "print(\"\\nSample dictionary entries:\")\n",
    "print(dict(list(word_index.items())[0:10]))\n",
    "\n",
    "# uncomment the following line to view the entire dictionary (which is very long!)\n",
    "# print(word_index) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Tokenize the review text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add/update the NLTK word list\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize each review in the dataset \n",
    "# then map each token to an index and store the results in dataset_reviews_tokens_index\n",
    "# use the word_tokenize from the nltk package to perform a quick tokenization.\n",
    "# dataset_reviews_tokens_index is a list that will contain the list of token indexes of each reviews \n",
    "\n",
    "dataset_index = list()\n",
    "for text in dataset_text:\n",
    "    #tokenize each review\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens_index = list()\n",
    "    #iterate through indexes\n",
    "    for token in tokens:\n",
    "        #check if token exist in word_index\n",
    "        if token in word_index:\n",
    "            # append the index of the token to the list tokens_index, \n",
    "            # which contains the list of indexes of each review\n",
    "            tokens_index.append(word_index[token])\n",
    "        #if token doesn't exist in word_index then we ignore it.  \n",
    "        else:\n",
    "            continue\n",
    "    # append the list of tokens_index to dataset_reviews_tokens_index\n",
    "    dataset_index.append(tokens_index)\n",
    "\n",
    "print(\"Original review: \" + str(dataset_text[0]))\n",
    "print(\"\\nIndexed review: \" + str(dataset_index[0]))\n",
    "print(dataset_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Truncate the review text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_padded = keras.preprocessing.sequence.pad_sequences(dataset_index,\n",
    "                                                        value=word_index[\"<PAD>\"],\n",
    "                                                        padding='post',\n",
    "                                                        maxlen=256)\n",
    "\n",
    "print(\"This review has \" + str(len(dataset_index[0])) + \" words.\")\n",
    "print(\"Indexed review:  \" + str(dataset_index[0]))\n",
    "\n",
    "print(\"\\nThis review has \" + str(len(dataset_padded[0])) + \" words.\")\n",
    "print(\"Padded review:  \" + str(dataset_padded[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Create the training and testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing data using the sklearn package\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_x, test_x, train_y, test_y = train_test_split(dataset_padded, dataset_labels, test_size=0.3)\n",
    "print(\"Ready\")\n",
    "print(train_x[0])\n",
    "print(test_x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training \n",
    "vocab_size = 88588 \n",
    "\n",
    "# Create an empty sequential model \n",
    "model = keras.Sequential()\n",
    "\n",
    "# Add input layer\n",
    "model.add(keras.layers.Embedding(vocab_size, 16))\n",
    "\n",
    "# Add a GlobalAveragePooling1D layer to the model\n",
    "model.add(keras.layers.GlobalAveragePooling1D())\n",
    "\n",
    "# Add a Dense layer to the model with 16 nodes\n",
    "# tf.nn.relu Computes rectified linear: max(features, 0).\n",
    "model.add(keras.layers.Dense(16, activation=tf.nn.relu))\n",
    "\n",
    "# Add an output layer to the model with one single node \n",
    "# tf.nn.sigmoid Computes sigmoid of x element-wise Specifically, y = 1 / (1 + exp(-x)).\n",
    "model.add(keras.layers.Dense(1, activation=tf.nn.sigmoid))\n",
    "\n",
    "# Display the summary of the model \n",
    "model.summary()\n",
    "print(\"Ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Define the accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimizer as adam \n",
    "# Define the loss function as a binary_crossentropy\n",
    "# Define the performance metric as the accuracy of the model (rate of correct guesses)\n",
    "model.compile(optimizer='adam',\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['acc'])\n",
    "print(\"Ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Define the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the size of the validation set\n",
    "validation_size = 10000 \n",
    "\n",
    "# extract the validation set from the training set \n",
    "x_val = train_x[:validation_size]\n",
    "\n",
    "# extract the rest of the data as the training set \n",
    "partial_x_train = train_x[validation_size:]\n",
    "\n",
    "# extract the labels of the validation set from the training set \n",
    "y_val = train_y[:validation_size]\n",
    "\n",
    "# extract the rest of the labels as the training labels \n",
    "partial_y_train = train_y[validation_size:]\n",
    "\n",
    "print(\"Ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Run the training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training on the training partial_x_train data \n",
    "# We include the validation data as input as well \n",
    "\n",
    "partial_y_train  = np.array(partial_y_train)\n",
    "y_val = np.array(y_val)\n",
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply model on testing data \n",
    "test_y = np.array(test_y)\n",
    "\n",
    "results = model.evaluate(test_x, test_y)\n",
    "#print(results)\n",
    "#print(type(results))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "a = model.predict_classes(test_x, verbose=0)\n",
    "confusion_matrix(test_y, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13: Visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss values for both training and validation\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "history_dict = history.history\n",
    "acc = history_dict['acc']\n",
    "val_acc = history_dict['val_acc']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "plt.clf() \n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy of the training and validation epochs\n",
    "\n",
    "plt.clf()  \n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
